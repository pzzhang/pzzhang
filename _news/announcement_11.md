---
layout: post
date: 2025-04-05 15:59:00-0400
inline: true
---

Llama 4 was released and open-sourced. I'm proud to continue leading the visual grounding effort from Llama 3 to Llama 4. We implemented state-of-the-art input-side and output-side visual grounding capabilities in Llama 3. Llama 4 achieved state-of-the-art performance on the Visual Commonsense Reasoning benchmark and the RefCoCo benchmark. More importantly, it works in real world scenarios and has powered several product features. Expert image grounding is highlighted as a <a href="https://ai.meta.com/blog/meta-llama-3/">key differentiator</a> for Llama 4.

- Blog post: <a href="https://www.llama.com/">Meta AI â€” Llama 4 announcement</a>
- Paper: no paper... But some technical details were discussed in this CVPR keynote talk <a href="https://cvpr.thecvf.com/virtual/2025/invited-talk/35403">The Llama Herd of Models: System 1, 2, 3 Go!</a>. Image grounding was discussed starting from !24:47. 
- Docs / model card: <a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama4/">Llama 4 model card</a>

Thanks to all collaborators and contributors on this project.
