<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>pzzhang</title>
    <meta name="author" content="Pengchuan  Zhang" />
    <meta name="description" content="I'm AI research scientist at Meta AI for VR and an affiliate assistant professor in the department of Electrical Engineering, University of Washington. I was a principal researcher at Microsoft Research, Redmond. Before joining Microsoft, I obtained my PhD degree in Applied and Computational Mathematics from Caltech in 2017. My research interests are mainly in the areas of deep learning, computer vision, multimodal intelligence, and theoretical foundations for deep learning. 
" />
    <meta name="keywords" content="Pengchuan, Pengchuan Zhang, deep learning, Mathematics" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸ”¥</text></svg>">
    <link rel="stylesheet" href="/pzzhang/assets/css/main.css">
    <link rel="canonical" href="https://pzzhang.github.io/pzzhang/">

    <!-- Dark Mode -->
    <script src="/pzzhang/assets/js/theme.js"></script>
    <script src="/pzzhang/assets/js/dark_mode.js"></script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/pzzhang/">about<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/pzzhang/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/pzzhang/publications/">publications</a>
              </li>
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/pzzhang/publications/">publications</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/pzzhang/projects/">projects</a>
                </div>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/pzzhang/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Pengchuan  Zhang</span>
          </h1>
          <p class="desc"></p>
        </header>

        <article>
          <div class="profile float-right">
<figure>

  <picture>
    <!-- Fallback to the original file -->
    <img class="img-fluid z-dept-1 rounded" src="https://raw.githubusercontent.com/pzzhang/pzzhang/master/assets/img/prof_pic-480.webp" alt="prof_pic.jpg">

  </picture>

</figure>

            <div class="address">
              <p>Meta Superintelligence Labs@Meta</p> <p>Menlo Park, CA 94025</p> <p>United States</p>

            </div>
          </div>

          <div class="clearfix">
            <p>Iâ€™m an AI research scientist at Segment Anything team of Meta Superintelligence Labs (previous FAIR computer vision team) and an affiliate assistant professor in the department of Electrical Engineering, University of Washington. I was a principal researcher at Microsoft Research, Redmond. Before joining Microsoft, I obtained my PhD degree in Applied and Computational Mathematics from Caltech in 2017. My research interests are mainly in the areas of deep learning, computer vision, multimodal intelligence, and theoretical foundations for deep learning.</p>

          </div>

          <!-- News -->          
          <div class="news">
            <h2>news</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <th scope="row">Nov 19, 2025</th>
                  <td>
                    Iâ€™m excited to share SAM 3, a unified model that enables detection, segmentation and tracking of objects across images and videos. SAM 3 introduces some of our most requested features like using text and exemplar prompts to segment all objects of a target category. We released both the model weights and a new open-vocab detection, segmentation and tracking benchmark with permissive license.

Try it out:
<ul>
  <li>Demo: <a href="https://aidemos.meta.com/segment-anything" target="_blank" rel="noopener noreferrer">Segment Anything Playground</a>
</li>
  <li>Model and benchmark: <a href="https://github.com/facebookresearch/sam3" target="_blank" rel="noopener noreferrer">SAM 3 Github repo</a>
</li>
  <li>Paper: <a href="https://ai.meta.com/research/publications/sam-3-segment-anything-with-concepts/" target="_blank" rel="noopener noreferrer">SAM 3: Segment Anything with Concepts</a>
</li>
  <li>Website: <a href="https://ai.meta.com/sam3/" target="_blank" rel="noopener noreferrer">SAM 3 official website</a>
</li>
</ul>

Thanks to all collaborators and contributors on this project.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Apr 5, 2025</th>
                  <td>
                    Llama 4 was released and open-sourced. Iâ€™m proud to continue leading the visual grounding effort from Llama 3 to Llama 4. We implemented state-of-the-art input-side and output-side visual grounding capabilities in Llama 3. Llama 4 achieved state-of-the-art performance on the Visual Commonsense Reasoning benchmark and the RefCoCo benchmark. More importantly, it works in real world scenarios and has powered several product features. Expert image grounding is highlighted as a <a href="https://ai.meta.com/blog/meta-llama-3/" target="_blank" rel="noopener noreferrer">key differentiator</a> for Llama 4.

<ul>
  <li>Blog post: <a href="https://www.llama.com/" target="_blank" rel="noopener noreferrer">Meta AI â€” Llama 4 announcement</a>
</li>
  <li>Paper: no paperâ€¦ But some technical details were discussed in this CVPR keynote talk <a href="https://cvpr.thecvf.com/virtual/2025/invited-talk/35403" target="_blank" rel="noopener noreferrer">The Llama Herd of Models: System 1, 2, 3 Go!</a>. Image grounding was discussed starting from !24:47.</li>
  <li>Docs / model card: <a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama4/" target="_blank" rel="noopener noreferrer">Llama 4 model card</a>
</li>
</ul>

Thanks to all collaborators and contributors on this project.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Jul 31, 2024</th>
                  <td>
                    Llama 3 was released and open-sourced. Iâ€™m proud to be a core contributor to Llama 3 and to have led the visual grounding effort. We implemented state-of-the-art input-side visual grounding capabilities in Llama 3. Under the anonymous name â€œOV-Groundingâ€, Llama 3 is the first model to reach human performance on the Visual Commonsense Reasoning leaderboard.

<ul>
  <li>Blog post: <a href="https://ai.meta.com/blog/meta-llama-3/" target="_blank" rel="noopener noreferrer">Meta AI â€” Llama 3 announcement</a>
</li>
  <li>Paper: <a href="https://arxiv.org/abs/2407.21783" target="_blank" rel="noopener noreferrer">arXiv:2407.21783</a>
</li>
  <li>Docs / model card: <a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/" target="_blank" rel="noopener noreferrer">Llama 3 model card</a>
</li>
  <li>Leaderboard: <a href="https://visualcommonsense.com/leaderboard/" target="_blank" rel="noopener noreferrer">Visual Commonsense Reasoning leaderboard</a>
</li>
</ul>

Thanks to all collaborators and contributors on this project.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Jun 17, 2024</th>
                  <td>
                    Two papers from our group received awards at CVPR 2024 â€” congratulations to the authors and collaborators!

<ol>
  <li>
<a href="https://shramanpramanick.github.io/EgoVLPv2/" target="_blank" rel="noopener noreferrer">EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone</a> was awarded as an <a href="https://egovis.github.io/awards/2022_2023/" target="_blank" rel="noopener noreferrer">EgoVis (Egocentric Vision) 2022/2023 Distinguished Paper</a>!</li>
  <li>
<a href="https://linzhiqiu.github.io/papers/genai_bench/" target="_blank" rel="noopener noreferrer">GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual Generation</a> won Best Short Paper Award at <a href="https://syndata4cv.github.io/" target="_blank" rel="noopener noreferrer">SynData@CVPR2024</a>!</li>
</ol>
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Oct 22, 2022</th>
                  <td>
                    Our ECCV 2022 workshop â€œComputer Vision in the Wildâ€ will take place on October 22, 2022. See the workshop site: <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank" rel="noopener noreferrer">Computer Vision in the Wild â€” ECCV 2022</a>.

Schedule (local times):

<ul>
  <li>Israel: 09:00â€“18:00</li>
  <li>Pacific Time: 23:00 (Oct 21)â€“08:00 (Oct 22)</li>
  <li>Beijing: 14:00â€“23:00</li>
</ul>

I will be chairing the morning session. Please join us!
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

          <!-- Selected papers -->
          <div class="publications">
            <h2>selected publications</h2>
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="lang2019using" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Using statistics to automate stochastic optimization</div>
          <!-- Author -->
          <div class="author">Lang, Hunter,Â Xiao, Lin,Â and <em>Zhang, Pengchuan</em>
                
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Advances in Neural Information Processing Systems</em> 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="zhang2021vinvl" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Vinvl: Revisiting visual representations in vision-language models</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Pengchuan</em>,Â Li, Xiujun,Â Hu, Xiaowei,Â Yang, Jianwei,Â Zhang, Lei,Â Wang, Lijuan,Â Choi, Yejin,Â and Gao, Jianfeng
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="salman2019convex" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A convex relaxation barrier to tight robustness verification of neural networks</div>
          <!-- Author -->
          <div class="author">Salman, Hadi,Â Yang, Greg,Â Zhang, Huan,Â Hsieh, Cho-Jui,Â and <em>Zhang, Pengchuan</em>
                
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint arXiv:1902.08722</em> 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="xu2018attngan" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</div>
          <!-- Author -->
          <div class="author">Xu, Tao,Â 
                  <em>Zhang, Pengchuan</em>,Â Huang, Qiuyuan,Â Zhang, Han,Â Gan, Zhe,Â Huang, Xiaolei,Â and He, Xiaodong
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the IEEE conference on computer vision and pattern recognition</em> 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="salman2019provably" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Provably robust deep learning via adversarially trained smoothed classifiers</div>
          <!-- Author -->
          <div class="author">Salman, Hadi,Â Yang, Greg,Â Li, Jerry,Â 
                  <em>Zhang, Pengchuan</em>,Â Zhang, Huan,Â Razenshteyn, Ilya,Â and Bubeck, Sebastien
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint arXiv:1906.04584</em> 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="zhang2021multi" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Pengchuan</em>,Â Dai, Xiyang,Â Yang, Jianwei,Â Xiao, Bin,Â Yuan, Lu,Â Zhang, Lei,Â and Gao, Jianfeng
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint arXiv:2103.15358</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="zhang2021multiscale" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Multiscale Invertible Generative Networks for High-Dimensional Bayesian Inference</div>
          <!-- Author -->
          <div class="author">Zhang, Shumao,Â 
                  <em>Zhang, Pengchuan</em>,Â and Hou, Thomas Y
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint arXiv:2105.05489</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="yuan2021florence" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Florence: A New Foundation Model for Computer Vision</div>
          <!-- Author -->
          <div class="author">Yuan, Lu,Â Chen, Dongdong,Â Chen, Yi-Ling,Â Codella, Noel,Â Dai, Xiyang,Â Gao, Jianfeng,Â Hu, Houdong,Â Huang, Xuedong,Â Li, Boxin,Â Li, Chunyuan,Â and others, 
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint arXiv:2111.11432</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="li2021grounded" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Grounded Language-Image Pre-training</div>
          <!-- Author -->
          <div class="author">Li, Liunian Harold,Â 
                  <em>Zhang, Pengchuan</em>,Â Zhang, Haotian,Â Yang, Jianwei,Â Li, Chunyuan,Â Zhong, Yiwu,Â Wang, Lijuan,Â Yuan, Lu,Â Zhang, Lei,Â Hwang, Jenq-Neng,Â and others, 
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint arXiv:2112.03857</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="pramanick2023egovlpv2" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Egovlpv2: Egocentric video-language pre-training with fusion in the backbone</div>
          <!-- Author -->
          <div class="author">Pramanick, Shraman,Â Song, Yale,Â Nag, Sayan,Â Lin, Kevin Qinghong,Â Shah, Hardik,Â Shou, Mike Zheng,Â Chellappa, Rama,Â and <em>Zhang, Pengchuan</em>
                
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="lin2023univtg" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Univtg: Towards unified video-language temporal grounding</div>
          <!-- Author -->
          <div class="author">Lin, Kevin Qinghong,Â 
                  <em>Zhang, Pengchuan</em>,Â Chen, Joya,Â Pramanick, Shraman,Â Gao, Difei,Â Wang, Alex Jinpeng,Â Yan, Rui,Â and Shou, Mike Zheng
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="chen2023minigpt" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Minigpt-v2: large language model as a unified interface for vision-language multi-task learning</div>
          <!-- Author -->
          <div class="author">Chen, Jun,Â Zhu, Deyao,Â Shen, Xiaoqian,Â Li, Xiang,Â Liu, Zechun,Â 
                  <em>Zhang, Pengchuan</em>,Â Krishnamoorthi, Raghuraman,Â Chandra, Vikas,Â Xiong, Yunyang,Â and Elhoseiny, Mohamed
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint arXiv:2310.09478</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="dubey2024llama" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">The llama 3 herd of models</div>
          <!-- Author -->
          <div class="author">Dubey, Abhimanyu,Â Jauhri, Abhinav,Â Pandey, Abhinav,Â Kadian, Abhishek,Â Al-Dahle, Ahmad,Â Letman, Aiesha,Â Mathur, Akhil,Â Schelten, Alan,Â Yang, Amy,Â Fan, Angela,Â and others, 
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv e-prints</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="lin2024evaluating" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Evaluating text-to-visual generation with image-to-text generation</div>
          <!-- Author -->
          <div class="author">Lin, Zhiqiu,Â Pathak, Deepak,Â Li, Baiqi,Â Li, Jiayao,Â Xia, Xide,Â Neubig, Graham,Â 
                  <em>Zhang, Pengchuan</em>,Â and Ramanan, Deva
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
</ol>
          </div>

          <!-- Social -->
          <div class="social">
            <div class="contact-icons">
            <a href="mailto:%70%65%6E%67%63%68%75%61%6E%7A%68%61%6E%67@%66%62.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://orcid.org/0000-0003-1155-9507" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=3VZ_E64AAAAJ&amp;hl" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/pzzhang" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/pengchuan-zhang-28ba9066" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/PengchuanZ" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
            <a href="https://scholar.google.com/citations?user=3VZ_E64AAAAJ&amp;hl=en" title="Work" target="_blank" rel="noopener noreferrer"><i class="fas fa-briefcase"></i></a>
            <a href="https://pzzhang.github.iopzzhang/feed.xml" title="RSS Feed" target="_blank" rel="noopener noreferrer"><i class="fas fa-rss-square"></i></a>
            
            </div>

            <div class="contact-note">
              Please reach me via my email pengchuanzhang@fb.com 

            </div>
            
          </div>
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2025 Pengchuan  Zhang. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Mansory & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/pzzhang/assets/js/mansory.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/pzzhang/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/pzzhang/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

